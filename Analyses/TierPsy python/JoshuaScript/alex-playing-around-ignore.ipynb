{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import OrderedDict\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "fps = 30\n",
    "seconds = 60\n",
    "\n",
    "folder_dict = OrderedDict({\n",
    "    'Unc-43 unpaired' : r'Z:\\Associative Conditioning Behavior Videos_2017 to 2018\\Alex Loftus - Put your videos here\\Psychfest\\feat_manual_files\\unc-43 unpaired',\n",
    "    'Unc-43 paired' : r'Z:\\Associative Conditioning Behavior Videos_2017 to 2018\\Alex Loftus - Put your videos here\\Psychfest\\feat_manual_files\\unc-43 paired',\n",
    "    'N2 unpaired' : r'Z:\\Associative Conditioning Behavior Videos_2017 to 2018\\Alex Loftus - Put your videos here\\Psychfest\\feat_manual_files\\wt unpaired',\n",
    "    'N2 paired' : r'Z:\\Associative Conditioning Behavior Videos_2017 to 2018\\Alex Loftus - Put your videos here\\Psychfest\\feat_manual_files\\wt paired',\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define functions necessary to parse worm data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseWormMovement(wormId, curWorm):\n",
    "    '''\n",
    "    Converts frames into seconds for a single worm. Parse a set of frames into average movement data per second.\n",
    "    '''\n",
    "    frame = 0  # Which frame within current second we're on\n",
    "    second = 0  # Which second we're on (0-indexed)\n",
    "    netMovement = 0\n",
    "    worm = [wormId] #create a new list starting with wormID\n",
    "        \n",
    "    for i in range(0, curWorm['worm_index'].size):\n",
    "        if(second == seconds):\n",
    "            break\n",
    "        \n",
    "        if(frame < fps):  # Potential off by 1?\n",
    "            if curWorm['motion_modes'].iloc[i] == 1.0:\n",
    "                netMovement += 1\n",
    "            if curWorm['motion_modes'].iloc[i] == -1.0:\n",
    "                netMovement -= 1\n",
    "        else:\n",
    "            frame = -1;\n",
    "            second += 1\n",
    "            worm.append(netMovement / fps) #append next 1sec average\n",
    "            netMovement = 0     \n",
    "        frame += 1\n",
    "        \n",
    "    #Populate remaining cells with NaN, Tierspy does not always output data for a worm for the entire video\n",
    "    for i in range(len(worm)-1, seconds):\n",
    "        worm.append(np.NaN)\n",
    "        \n",
    "    return worm\n",
    "#end parseWormMovement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parseFile(filename):\n",
    "    '''\n",
    "    Return a DataFrame with motion mode data. Uses 'ParseWormMovement' and loops it on all the worms to get a new DataFrame.\n",
    "    '''\n",
    "    hdf = pd.HDFStore(filename)\n",
    "    wormIds = [] #list of unique ids in an easily iterable format\n",
    "    wormData= [] #structure for final data to be pushed data frame, then excel\n",
    "    \n",
    "    ts = hdf['features_timeseries'] #all worm data\n",
    "    uniqueWorms = ts.drop_duplicates('worm_index') #only one entry per wormID\n",
    "    wormCount = uniqueWorms['worm_index'].size\n",
    "\n",
    "    #Resets indexes to be continuous. Drop=True prevents the creation of a new index column\n",
    "    uniqueWorms.reset_index(drop=True)\n",
    "\n",
    "    #populate list of unique worm IDs\n",
    "    for i in range(0, wormCount):\n",
    "        wormIds.append(uniqueWorms.iloc[i,0])\n",
    "\n",
    "    # Run on each individual worm\n",
    "    for id in wormIds:\n",
    "        curWorm = ts.loc[ts['worm_index'] == id]\n",
    "        wormData.append(parseWormMovement(id, curWorm))\n",
    "\n",
    "    column_names = ['Worm ID']\n",
    "    for i in range(0, seconds):\n",
    "        column_names.append('{}'.format(i+1))\n",
    "\n",
    "    return pd.DataFrame(wormData, columns=column_names).round(2)  # Final DataFrame before excel\n",
    "#end parseFile\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making a big DF for all worm data files"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define functions to make big dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_big_df(folder_name):\n",
    "    '''Concatanate a bunch of different disparate files into one DataFrame. Uses ParseFile and loops on files in a single folder.'''\n",
    "    file_list = [(folder_name + '\\\\' + i) for i in os.listdir(folder_name)]\n",
    "    return pd.concat([parseFile(file_list[i]) for i in range(len(file_list))], axis=0, keys=[i[:-17] for i in os.listdir(folder_name)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def avg_movement(big_df):\n",
    "    '''Return transposed dataframe with worm movement averaged in each file. This function merges all the worms into one averaged value.'''\n",
    "    return big_df.mean(level=0).drop('Worm ID', axis=1).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def concat_dates(folder_name):  \n",
    "    '''\n",
    "    Return list of DataFrames. Merges all the disparate dates together into one, and returns something only separated by minutes (e.g. 1min, 5min, and 10min data for each group type)\n",
    "    \n",
    "    df[0] : Pre data.\n",
    "    df[1] : Post data.\n",
    "    '''\n",
    "    df = avg_movement(make_big_df(folder_name))\n",
    "    \n",
    "    def split_prepost(df2):  # Split pre and post data into two different DataFrames.\n",
    "        bdf_pre = df2[[i for i in df2 if 'Pre' in i]][0:10]  # Make DataFrame with only pre-stim data\n",
    "        bdf_post = df2[[i for i in df2 if 'Post' in i]][0:10] # Make DataFrame with only post-stim data\n",
    "        \n",
    "        return (bdf_pre, bdf_post)  # Return tuple with pre and post data\n",
    "    \n",
    "    list_of_dfs = []\n",
    "    for j in split_prepost(df):  # consolidate dates\n",
    "        j['1min'] = j[[i for i in j.columns if '1min' in i]].mean(axis=1)\n",
    "        j['5min'] = j[[i for i in j.columns if '5min' in i]].mean(axis=1)\n",
    "        j['10min'] = j[[i for i in j.columns if '10min' in i]].mean(axis=1)\n",
    "        a = j[['1min', '5min', '10min']]\n",
    "        list_of_dfs.append(a)\n",
    "    \n",
    "    return list_of_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def send_dfs_to_excel(cd, output):\n",
    "    '''Take in output from concat_dates, and makes an Excel file.'''\n",
    "    bdf_means_pre = cd[0] # DataFrame with just Pre data\n",
    "    bdf_means_post = cd[1] # DataFrame with just Post data\n",
    "\n",
    "    writer = pd.ExcelWriter(output, engine='xlsxwriter')\n",
    "    bdf_means_pre.to_excel(writer, sheet_name='Pre')\n",
    "    bdf_means_post.to_excel(writer, sheet_name='Post')\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loop through to do this for a bunch of excel files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "for (intg, folder) in enumerate(folder_dict.values()):\n",
    "    send_dfs_to_excel(concat_dates(folder), (list(folder_dict.keys())[intg] + '.xlsx'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Playing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = make_big_df(r'Z:\\Associative Conditioning Behavior Videos_2017 to 2018\\Alex Loftus - Put your videos here\\Psychfest\\feat_manual_files\\unc-43 unpaired')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('V3 2.19.18 VC1052 Unpaired ARL 10min Post', 0)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#df.index[0]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
